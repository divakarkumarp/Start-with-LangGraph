{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLaC8_iSnr6F"
      },
      "source": [
        "[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
        "\n",
        "Here we are :\n",
        "* Create a Chatbots With Langgraph\n",
        "\n",
        "We will use state-of-the-art open models from Google for LLM -> *Gemma2-9b-It*  for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEnQfmJ2nskj",
        "outputId": "3fcfe77f-ebdf-4ef0-cc92-47dec7954510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.10/dist-packages (0.1.104)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langgraph) (0.2.34)\n",
            "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith) (3.10.7)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langsmith) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langsmith) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (2.0.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.27->langgraph) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzX9IIRcnzC0",
        "outputId": "a92507d0-3d2f-4c7b-f5c6-fbd8fed1ca43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.34)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.104)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.9.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_groq langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UprqwWmQn2JU",
        "outputId": "e8231b9c-79d9-4cdc-a4f3-d7a0fe8b3428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls__0a667f0b1a484a1e827f23f77479c801\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key=userdata.get('groq_api_key')\n",
        "langsmith=userdata.get('LANGSMITH_API_KEY')\n",
        "print(langsmith)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lrd7GoddonZb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]=\"TestLanggraph\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OqwjSEGPos34"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOD9Z_qzovDC",
        "outputId": "ca47f6fa-81c9-45e0-ef56-cb9486b01816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x78310d093b80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x78310d0e03a0>, model_name='Gemma2-9b-It', groq_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Gemma2-9b-It\")\n",
        "llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdlIlVz8889R"
      },
      "source": [
        "Let's Building Chatbot Using Langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tiXdaQdqoz7A"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph,START,END\n",
        "from langgraph.graph.message import add_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zwfjoT869Ds5"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "  # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "  messages:Annotated[list,add_messages]\n",
        "\n",
        "# Initialize the StateGraph with the defined State structure\n",
        "graph_builder=StateGraph(State)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgRmtqFC9Glp",
        "outputId": "457d5ea1-2ffa-4e31-d2ae-9f6f2bf743d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x78310d3f1960>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xDWUPhX49w5l"
      },
      "outputs": [],
      "source": [
        "def chatbot(state:State):\n",
        "    # This function takes the current state as input and returns the updated state with new messages.\n",
        "    # The 'llm.invoke' function is called with the current list of messages from the state,\n",
        "    # generating a response which is then returned as part of the updated state.\n",
        "  return {\"messages\":llm.invoke(state['messages'])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SPBGe3-69zRR"
      },
      "outputs": [],
      "source": [
        "# Add the 'chatbot' function as a node to the state graph.\n",
        "# This node can be invoked in the state graph to process the state and update it using the chatbot function.\n",
        "graph_builder.add_node(\"chatbot\",chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O9UGOFQ91l9",
        "outputId": "1fd1c329-3629-4ee3-b6e5-eee285a29c48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x78310d3f1960>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Wvo3jCo394Gp"
      },
      "outputs": [],
      "source": [
        "# Define the flow of the state graph.\n",
        "# Start by connecting the START node to the 'chatbot' node.\n",
        "graph_builder.add_edge(START,\"chatbot\")\n",
        "# Then, connect the 'chatbot' node to the END node, marking the completion of the state transition.\n",
        "graph_builder.add_edge(\"chatbot\",END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "z4VfrSHy96U5"
      },
      "outputs": [],
      "source": [
        "# Compile the state graph, finalizing its structure.\n",
        "# This prepares the graph for execution by setting up the nodes and edges as defined earlier.\n",
        "graph=graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "3Mdx1xpz98SW",
        "outputId": "953bf128-96db-408e-c3d7-e207c4b7ccaf"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "try:\n",
        "    # Attempt to generate and display a visual representation of the compiled state graph.\n",
        "    # The graph is rendered as a PNG image using Mermaid and then displayed in the output.\n",
        "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "  # If there is any issue during the graph generation or display, it will be silently ignored.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg3jLa0r9-UI",
        "outputId": "03d142e2-2c17-4c4b-e140-3a45d59a2eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello\n",
            "dict_values([{'messages': AIMessage(content='Hello! ðŸ‘‹\\n\\nHow can I help you today? ðŸ˜Š\\n', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 10, 'total_tokens': 25, 'completion_time': 0.027272727, 'prompt_time': 4.7e-07, 'queue_time': 0.014327557999999999, 'total_time': 0.027273197}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-780b02d0-abd3-410c-9895-b35c7e9cdbf8-0', usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25})}])\n",
            "content='Hello! ðŸ‘‹\\n\\nHow can I help you today? ðŸ˜Š\\n' response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 10, 'total_tokens': 25, 'completion_time': 0.027272727, 'prompt_time': 4.7e-07, 'queue_time': 0.014327557999999999, 'total_time': 0.027273197}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-780b02d0-abd3-410c-9895-b35c7e9cdbf8-0' usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25}\n",
            "Assistant: Hello! ðŸ‘‹\n",
            "\n",
            "How can I help you today? ðŸ˜Š\n",
            "\n",
            "User: What is Transformer\n",
            "dict_values([{'messages': AIMessage(content=\"A Transformer is a powerful deep learning model architecture primarily used for natural language processing (NLP) tasks. \\n\\n**Here's a breakdown:**\\n\\n**What it does:**\\n\\n* **Processes sequential data:** Transformers excel at understanding and generating text, code, or any other data with a natural order (like time series).\\n\\n* **Handles long-range dependencies:** Unlike earlier models, Transformers can effectively capture relationships between words that are far apart in a sentence, leading to better comprehension of complex contexts.\\n\\n* **Parallelizes training:**  Transformers can be trained much faster than traditional recurrent neural networks (RNNs) because many operations can be performed simultaneously.\\n\\n**Key Features:**\\n\\n* **Attention Mechanism:** The heart of a Transformer is its attention mechanism.  It allows the model to focus on specific parts of the input sequence that are most relevant to the task at hand. Think of it like a spotlight that highlights important words.\\n\\n* **Encoder-Decoder Structure:**  Transformers usually consist of an encoder and a decoder.\\n    * **Encoder:**  Processes the input sequence, understanding its meaning.\\n    * **Decoder:**  Generates the output sequence based on the encoder's understanding.\\n\\n* **Positional Encodings:**  Since Transformers don't process data sequentially like RNNs, they need a way to know the order of words.  Positional encodings are added to the input embeddings to provide this information.\\n\\n**Impact:**\\n\\nTransformers have revolutionized NLP, leading to significant improvements in:\\n\\n* **Machine Translation:**  Producing more accurate and fluent translations.\\n* **Text Summarization:**  Generating concise and informative summaries of large texts.\\n* **Question Answering:**  Providing accurate answers to questions based on given context.\\n* **Text Generation:**  Creating human-like text for various applications, such as chatbots and story writing.\\n\\n**Popular Transformer Models:**\\n\\n* BERT (Bidirectional Encoder Representations from Transformers)\\n* GPT (Generative Pre-trained Transformer)\\n* T5 (Text-to-Text Transfer Transformer)\\n* LaMDA (Language Model for Dialogue Applications)\\n\\n**In essence, Transformers are powerful tools that have significantly advanced the field of natural language processing.**\\n\", response_metadata={'token_usage': {'completion_tokens': 456, 'prompt_tokens': 12, 'total_tokens': 468, 'completion_time': 0.829090909, 'prompt_time': 7.704e-05, 'queue_time': 0.01280653, 'total_time': 0.829167949}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-58169626-2da1-49ee-b8a8-6236e3c77802-0', usage_metadata={'input_tokens': 12, 'output_tokens': 456, 'total_tokens': 468})}])\n",
            "content=\"A Transformer is a powerful deep learning model architecture primarily used for natural language processing (NLP) tasks. \\n\\n**Here's a breakdown:**\\n\\n**What it does:**\\n\\n* **Processes sequential data:** Transformers excel at understanding and generating text, code, or any other data with a natural order (like time series).\\n\\n* **Handles long-range dependencies:** Unlike earlier models, Transformers can effectively capture relationships between words that are far apart in a sentence, leading to better comprehension of complex contexts.\\n\\n* **Parallelizes training:**  Transformers can be trained much faster than traditional recurrent neural networks (RNNs) because many operations can be performed simultaneously.\\n\\n**Key Features:**\\n\\n* **Attention Mechanism:** The heart of a Transformer is its attention mechanism.  It allows the model to focus on specific parts of the input sequence that are most relevant to the task at hand. Think of it like a spotlight that highlights important words.\\n\\n* **Encoder-Decoder Structure:**  Transformers usually consist of an encoder and a decoder.\\n    * **Encoder:**  Processes the input sequence, understanding its meaning.\\n    * **Decoder:**  Generates the output sequence based on the encoder's understanding.\\n\\n* **Positional Encodings:**  Since Transformers don't process data sequentially like RNNs, they need a way to know the order of words.  Positional encodings are added to the input embeddings to provide this information.\\n\\n**Impact:**\\n\\nTransformers have revolutionized NLP, leading to significant improvements in:\\n\\n* **Machine Translation:**  Producing more accurate and fluent translations.\\n* **Text Summarization:**  Generating concise and informative summaries of large texts.\\n* **Question Answering:**  Providing accurate answers to questions based on given context.\\n* **Text Generation:**  Creating human-like text for various applications, such as chatbots and story writing.\\n\\n**Popular Transformer Models:**\\n\\n* BERT (Bidirectional Encoder Representations from Transformers)\\n* GPT (Generative Pre-trained Transformer)\\n* T5 (Text-to-Text Transfer Transformer)\\n* LaMDA (Language Model for Dialogue Applications)\\n\\n**In essence, Transformers are powerful tools that have significantly advanced the field of natural language processing.**\\n\" response_metadata={'token_usage': {'completion_tokens': 456, 'prompt_tokens': 12, 'total_tokens': 468, 'completion_time': 0.829090909, 'prompt_time': 7.704e-05, 'queue_time': 0.01280653, 'total_time': 0.829167949}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-58169626-2da1-49ee-b8a8-6236e3c77802-0' usage_metadata={'input_tokens': 12, 'output_tokens': 456, 'total_tokens': 468}\n",
            "Assistant: A Transformer is a powerful deep learning model architecture primarily used for natural language processing (NLP) tasks. \n",
            "\n",
            "**Here's a breakdown:**\n",
            "\n",
            "**What it does:**\n",
            "\n",
            "* **Processes sequential data:** Transformers excel at understanding and generating text, code, or any other data with a natural order (like time series).\n",
            "\n",
            "* **Handles long-range dependencies:** Unlike earlier models, Transformers can effectively capture relationships between words that are far apart in a sentence, leading to better comprehension of complex contexts.\n",
            "\n",
            "* **Parallelizes training:**  Transformers can be trained much faster than traditional recurrent neural networks (RNNs) because many operations can be performed simultaneously.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "* **Attention Mechanism:** The heart of a Transformer is its attention mechanism.  It allows the model to focus on specific parts of the input sequence that are most relevant to the task at hand. Think of it like a spotlight that highlights important words.\n",
            "\n",
            "* **Encoder-Decoder Structure:**  Transformers usually consist of an encoder and a decoder.\n",
            "    * **Encoder:**  Processes the input sequence, understanding its meaning.\n",
            "    * **Decoder:**  Generates the output sequence based on the encoder's understanding.\n",
            "\n",
            "* **Positional Encodings:**  Since Transformers don't process data sequentially like RNNs, they need a way to know the order of words.  Positional encodings are added to the input embeddings to provide this information.\n",
            "\n",
            "**Impact:**\n",
            "\n",
            "Transformers have revolutionized NLP, leading to significant improvements in:\n",
            "\n",
            "* **Machine Translation:**  Producing more accurate and fluent translations.\n",
            "* **Text Summarization:**  Generating concise and informative summaries of large texts.\n",
            "* **Question Answering:**  Providing accurate answers to questions based on given context.\n",
            "* **Text Generation:**  Creating human-like text for various applications, such as chatbots and story writing.\n",
            "\n",
            "**Popular Transformer Models:**\n",
            "\n",
            "* BERT (Bidirectional Encoder Representations from Transformers)\n",
            "* GPT (Generative Pre-trained Transformer)\n",
            "* T5 (Text-to-Text Transfer Transformer)\n",
            "* LaMDA (Language Model for Dialogue Applications)\n",
            "\n",
            "**In essence, Transformers are powerful tools that have significantly advanced the field of natural language processing.**\n",
            "\n",
            "User: What is Langgraph\n",
            "dict_values([{'messages': AIMessage(content=\"LangGraph is an open-source framework for building and evaluating large language models (LLMs). \\n\\nHere's a breakdown:\\n\\n**Key Features:**\\n\\n* **Modular Design:** LangGraph is designed in a modular way, allowing users to easily customize and experiment with different components of the LLM pipeline. This includes tokenizers, model architectures, training algorithms, and evaluation metrics.\\n* **Hardware Agnostic:**  It works across various hardware platforms, including CPUs, GPUs, and TPUs, making it accessible to a wider range of researchers and developers.\\n* **Parallel Training:** LangGraph supports parallel training techniques, which significantly accelerate the training process for large models.\\n* **Extensive Documentation and Community:**  It comes with comprehensive documentation and an active community, providing support and resources for users.\\n\\n**Benefits:**\\n\\n* **Flexibility:**  The modular nature of LangGraph allows researchers to tailor their LLMs to specific tasks or domains.\\n* **Efficiency:** Parallel training and optimization techniques help reduce training time and resource consumption.\\n* **Transparency:**  The open-source nature of LangGraph promotes transparency and collaboration in the LLM research community.\\n\\n**Use Cases:**\\n\\n* **Text Generation:**  Generating creative content, summarizing text, translating languages.\\n* **Dialogue Systems:**  Building chatbots and conversational agents.\\n* **Code Generation:**  Assisting in writing and understanding code.\\n* **Question Answering:**  Providing accurate answers to questions based on given context.\\n\\n**Getting Started:**\\n\\nYou can find more information about LangGraph, including installation instructions and tutorials, on the official website or GitHub repository.\\n\\n\\nLet me know if you have any other questions about LangGraph or large language models in general!\\n\", response_metadata={'token_usage': {'completion_tokens': 355, 'prompt_tokens': 13, 'total_tokens': 368, 'completion_time': 0.645454545, 'prompt_time': 8.0489e-05, 'queue_time': 0.014070341, 'total_time': 0.645535034}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-dabc631a-11de-47db-96ea-bbd37effa61c-0', usage_metadata={'input_tokens': 13, 'output_tokens': 355, 'total_tokens': 368})}])\n",
            "content=\"LangGraph is an open-source framework for building and evaluating large language models (LLMs). \\n\\nHere's a breakdown:\\n\\n**Key Features:**\\n\\n* **Modular Design:** LangGraph is designed in a modular way, allowing users to easily customize and experiment with different components of the LLM pipeline. This includes tokenizers, model architectures, training algorithms, and evaluation metrics.\\n* **Hardware Agnostic:**  It works across various hardware platforms, including CPUs, GPUs, and TPUs, making it accessible to a wider range of researchers and developers.\\n* **Parallel Training:** LangGraph supports parallel training techniques, which significantly accelerate the training process for large models.\\n* **Extensive Documentation and Community:**  It comes with comprehensive documentation and an active community, providing support and resources for users.\\n\\n**Benefits:**\\n\\n* **Flexibility:**  The modular nature of LangGraph allows researchers to tailor their LLMs to specific tasks or domains.\\n* **Efficiency:** Parallel training and optimization techniques help reduce training time and resource consumption.\\n* **Transparency:**  The open-source nature of LangGraph promotes transparency and collaboration in the LLM research community.\\n\\n**Use Cases:**\\n\\n* **Text Generation:**  Generating creative content, summarizing text, translating languages.\\n* **Dialogue Systems:**  Building chatbots and conversational agents.\\n* **Code Generation:**  Assisting in writing and understanding code.\\n* **Question Answering:**  Providing accurate answers to questions based on given context.\\n\\n**Getting Started:**\\n\\nYou can find more information about LangGraph, including installation instructions and tutorials, on the official website or GitHub repository.\\n\\n\\nLet me know if you have any other questions about LangGraph or large language models in general!\\n\" response_metadata={'token_usage': {'completion_tokens': 355, 'prompt_tokens': 13, 'total_tokens': 368, 'completion_time': 0.645454545, 'prompt_time': 8.0489e-05, 'queue_time': 0.014070341, 'total_time': 0.645535034}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-dabc631a-11de-47db-96ea-bbd37effa61c-0' usage_metadata={'input_tokens': 13, 'output_tokens': 355, 'total_tokens': 368}\n",
            "Assistant: LangGraph is an open-source framework for building and evaluating large language models (LLMs). \n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "* **Modular Design:** LangGraph is designed in a modular way, allowing users to easily customize and experiment with different components of the LLM pipeline. This includes tokenizers, model architectures, training algorithms, and evaluation metrics.\n",
            "* **Hardware Agnostic:**  It works across various hardware platforms, including CPUs, GPUs, and TPUs, making it accessible to a wider range of researchers and developers.\n",
            "* **Parallel Training:** LangGraph supports parallel training techniques, which significantly accelerate the training process for large models.\n",
            "* **Extensive Documentation and Community:**  It comes with comprehensive documentation and an active community, providing support and resources for users.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "* **Flexibility:**  The modular nature of LangGraph allows researchers to tailor their LLMs to specific tasks or domains.\n",
            "* **Efficiency:** Parallel training and optimization techniques help reduce training time and resource consumption.\n",
            "* **Transparency:**  The open-source nature of LangGraph promotes transparency and collaboration in the LLM research community.\n",
            "\n",
            "**Use Cases:**\n",
            "\n",
            "* **Text Generation:**  Generating creative content, summarizing text, translating languages.\n",
            "* **Dialogue Systems:**  Building chatbots and conversational agents.\n",
            "* **Code Generation:**  Assisting in writing and understanding code.\n",
            "* **Question Answering:**  Providing accurate answers to questions based on given context.\n",
            "\n",
            "**Getting Started:**\n",
            "\n",
            "You can find more information about LangGraph, including installation instructions and tutorials, on the official website or GitHub repository.\n",
            "\n",
            "\n",
            "Let me know if you have any other questions about LangGraph or large language models in general!\n",
            "\n",
            "User: q\n",
            "Good Bye\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  # Continuously prompt the user for input\n",
        "  user_input=input(\"User: \")\n",
        "\n",
        "  # Check if the user wants to quit the conversation\n",
        "  if user_input.lower() in [\"quit\",\"q\"]:\n",
        "    print(\"Good Bye\")\n",
        "    break\n",
        "\n",
        "  # Stream events through the graph using the user's input\n",
        "  for event in graph.stream({'messages':(\"user\",user_input)}):\n",
        "    # Print all event values (debugging/verification step)\n",
        "    print(event.values())\n",
        "\n",
        "    # Iterate through the event values to extract and print the assistant's response\n",
        "    for value in event.values():\n",
        "      print(value['messages'])# Prints the entire messages list\n",
        "      print(\"Assistant:\",value[\"messages\"].content)  # Prints the content of the assistant's response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7N_DHXNN-BFl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
